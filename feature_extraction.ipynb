{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((352, 1216)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # why normalize?\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directories\n",
    "base_dir = 'dataset'\n",
    "image_dir = os.path.join(base_dir, 'image')\n",
    "depth_dir = os.path.join(base_dir, 'groundtruth_depth')\n",
    "# Load Dataset\n",
    "image_files = sorted([os.path.join(image_dir, file) for file in os.listdir(image_dir)])\n",
    "depth_files = sorted([os.path.join(depth_dir, file) for file in os.listdir(depth_dir)])\n",
    "\n",
    "# Load the data into variables\n",
    "images = [ToTensor()(Image.open(file)) for file in image_files]\n",
    "depth_maps = [ToTensor()(Image.open(file)) for file in depth_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(removed_layers = 1):\n",
    "    # Load Pre-Trained Model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    #model = model.to(\"cpu\")\n",
    "    #remove final connected layers from model\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-removed_layers])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract features\n",
    "def extract_features(dataset, model = load_model(1)):\n",
    "    features = []\n",
    "    with torch.no_grad(): #Disables gradient calcullation\n",
    "        for input in dataset:\n",
    "            if input.dim() == 3:\n",
    "                input = input.unsqueeze(0)\n",
    "            outputs = model(input)\n",
    "            features.append(outputs.flatten(1))\n",
    "    return torch.cat(features)\n",
    "\n",
    "features = extract_features(images, load_model(1))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8934, 0.9360, 0.8536,  ..., 0.8746, 1.0203, 0.9104],\n",
       "        [0.8632, 0.9687, 0.9074,  ..., 0.8981, 1.0322, 0.9059],\n",
       "        [0.8677, 0.9218, 0.8741,  ..., 0.8962, 1.0086, 0.8647],\n",
       "        ...,\n",
       "        [0.8320, 0.9539, 0.8714,  ..., 0.8714, 1.0696, 0.9383],\n",
       "        [0.8638, 0.9748, 0.8518,  ..., 0.9089, 1.0226, 0.9441],\n",
       "        [0.8125, 0.9781, 0.8474,  ..., 0.8393, 1.0156, 0.8690]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Dimensionality with LLE\n",
    "1 512\n",
    "2 214016\n",
    "3 428032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 512]), (1000, 150))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparamaters!\n",
    "n_components = 150\n",
    "n_neighbors = 25\n",
    "lle = LocallyLinearEmbedding(n_components = n_components, n_neighbors = n_neighbors, method='standard')\n",
    "features_lle = lle.fit_transform(features)\n",
    "features.shape, features_lle.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
